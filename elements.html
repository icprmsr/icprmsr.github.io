<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title> Evaluation </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
					<!--	<a href="index.html" class="logo"> Evaluation </a> -->
					<img src="images/title.png" width="50%" align="center">
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html"> Description </a></li>
							<li><a href="dataset.html"> Dataset </a></li>
							<li class="active"><a href="elements.html"> Evaluation </a></li>
							<li><a href="challenge.html"> Challenge Tasks </a></li>
							<li><a href="deadline.html"> Deadline </a></li>
							<li><a href="organizers.html"> Organizers </a></li>
						</ul>
						
					</nav>
                   
				<!-- Main -->
					<div id="main">
                     
					<article class="post featured">
						<h3> Evaluation Criteria </h3>
						<p style="text-align:justify; text-justify:inter-ideograph;">
						The participants are required to output non-repeating subtitles and concatenate them
						for each video. For a given video, subtitles of adjacent frames may be the same, and
						the participants are required to deduplicate them and output subtitles. Normalized
						metric in terms of Normalized Edit Distance (1-N.E.D. specially) will be treated as
						the official ranking metric.
						We will evaluate the predicted transcription with
						the Normalized Edit Distance (N.E.D), which is formulated as:
						<br/>
						<img src="images/pic2.png" width="30%" align="center" />
						<br/>
						where D(:) stands for the Levenshtein Distance, and     and   denote the predicted
 							subtitles in string and the corresponding ground truths for each video. N is the
							 number of the test videoes.
					    <br/>
						<br>
						<b> Note: </b> To avoid the ambiguity in annotations, we preform preprocessing before
						evaluation: 1)The English letters are not case sensitive; 2) The Chinese traditional and
						simplified characters are treated as the same label; 3)The blank spaces and symbols
						will be removed; 4) All illegible videoes will not contribute to the evaluation result.
						
                        <br/>
						<h3> Evaluation Plan </h3>
						<p style="text-align:justify; text-justify:inter-ideograph;">
							On March 12, organizers provide the training set with annotations. Each task has 50 hours of video data. 
							Participants are required to develop corresponding models according to the requirements of each track.
                            <br/>
							<br>
							On April 22, the organizers will provide the validation set without annotations. And each track contains 20 hours of video data. 
							Participants predict subtitles of each video, and submit the prediction results to the CodaLab website. 
							The organizers will give the ranking on the validation set according to the prediction result.
                            <br/>
							<br>
							On May 7th, the organizers will provide the test set (including 5 hours of video data) without annotations. 
							It is required that the top ten participants of each track in the validation set be within two days (that is, before May 9th). 
							According to the video data in the test set, <b> top ten participants on the validation set </b> are required to predict the subtitles in the video, 
							and submit the prediction results to the CodaLab website.
							Finally, <b> the final rank of each track = 50% * rank on the validation set + 50% * rank on the test set </b>. 
							The test set will be announced on the official website (icprmsr.github.io) after May 9, 
							and the final ranking information of the participants will also be announced on the competition website, 
							and the participants will be notified by email.
						</p>
					</article>

					</div>


			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>