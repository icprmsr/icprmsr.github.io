<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title> Multimodal Subtitle Recognition </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Intro -->
					<div id="intro">
						<h2> <strong> Multimodal Subtitle Recognition </strong> </h2>
						<br>
						<p style="text-align:justify; text-justify:inter-ideograph;"> 
							In recent years, with the rapid rise of short videos, live broadcastsand other media-based applications, the widespread transmission of video data has led to a significant
							increase in user-generated content. 
							A wide variety of creative platforms and pattern have emerged, and media publication criteria is becoming more and more civilian,
							leading to more complex and dynamic acoustic scenes in various long or short video and live streaming. 
							The problems of video subtitle recognition and speech recognition under various scenarios have been of considerable concern by researchers. 
							The development of methods accurately recognize and understand various types of video content has become an indispensable tool in downstream applications such as subtitle creation, content recommendation, digital media
							archival, and so on. 
							In this challenge, we focus on extracting subtitles from videos using both visual and audio modalities. Most previous works exploit it with a single modality , and each modality has its own
							advantages and annotation difficulties on various types of training data. 
							To this end, we present tasks that explore the integration of the advantages of both video and audio modalitioes. 
							We expect to obtain high precision subtitles with lower annotation costs, thus expanding the scope of practical applications.
						</p>
						<ul class="actions">
							<li><a href="#header" class="button icon solid solo fa-arrow-down scrolly">Continue</a></li>
						</ul>
					</div>

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo"> Description </a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="active"><a href="index.html"> Description </a></li>
							<li><a href="dataset.html"> Dataset </a></li>
							<li><a href="elements.html"> Evaluation </a></li>
							<li><a href="challenge.html"> Challenge Tasks </a></li>
							<li><a href="deadline.html"> Deadline </a></li>
							<li><a href="organizers.html"> Organizers </a></li>
						</ul>
						<ul class="icons">
							<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
							<li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
							<li><a href="#" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Featured Post -->
							<article class="post featured">
							<h3>
								News and Updates
							</h3>
							<p style="text-align:justify; text-justify:inter-ideograph;">
								2022-03-04: Training and validation data has been released ( <a href="https://pan.baidu.com/s/12GtbAgjwd_Bpp3DxU94QFQ">Baidu Drive</a> (Extraction code: 0aka). 
								Paticipants can use the released data to develop their algorithms.
							</p>
							
							<h3> Description of the problem </h3>
                            <p style="text-align:justify; text-justify:inter-ideograph;"> 
								Videos carry rich information with both visual (including text) and audio modality.
								Video understanding has been a popular research topic in both academia and industry.
								The development of methods to fuse the multi-modal information is also a
								challenging and meaningful exploration.
								In this challenge, we focus on extracting subtitles from videos. Subtitles are the text
								derived from either a transcript or screenplay of the dialogue or commentary in
								videos. Subtitles may be the most important text information for video data, as they
								contain information of what the people said. They are widely used in
								recommendation, retrieval and video understanding systems.
								Subtitles refer to the text displayed in the video after converting the audio into
								accurate text, however the text in the video is not always belong to subtitles. For
								example, the text in the blue boxes in the figure is not a subtitle, but the text in the red
								box that converts audio into text is a subtitle.
							</p>
                            <p style="text-align:center;">
								<img src="images/pic0.png" width="50%" align="center" />
							</p>
                            <p style="text-align:justify; text-justify:inter-ideograph;"> 
								Subtitle extraction is a challenging task with single modal information. The audio
								method is sensitive to the background noise and variations in accent, and some
								specific or homophonic words are difficult to recognize accurately. If we only
								consider visual information, it is plausible that the above problems could be tackled.
								However, other problems along with video scenes appear if we overlook speech
								information: Texts of plenty of categories are included and stacked, such as logos,
								ads, backgrounds, and so on. They are great interference for extracting subtitles only
								from video frames.
								From the application perspective, annotating videos is very costly, whether in
								annotating audio or text. Sometimes, the annotations of a single modality may be
								available. In this challenge, we aim to encourage the development of methods to use
								this supervision to train models to learn to perform annotations in another modality,
								exploring new way of effective algorithm iteration across different modality.
								Therefore, fusing both audio and visual modalities is necessary
								and complimentary for subtitle extraction. In this challenge, we provide a large-scale
								video dataset with both visual and audio annotations for extracting subtitles with
								multi-modal technologies. We provide three subtasks for the participants, on the first
								task, participants can use only audio supervision, whereas in the second, only visual
								information is provided. In the third subtask, both visual and audio supervision are
								provided and can be used.
							</p>
							</article>
					</div>
			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>