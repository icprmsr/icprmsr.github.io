<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title> ICPRMSR </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Intro -->
					
					<header id="header">
						<!--   <a href="index.html" class="logo"> Description </a> -->
						<img src="images/title.png" width="50%" align="center">
					</header>
			

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="active"><a href="index.html"> Description </a></li>
							<li><a href="dataset.html"> Dataset </a></li>
							<li><a href="elements.html"> Evaluation </a></li>
							<li><a href="challenge.html"> Challenge Tasks </a></li>
							<li><a href="deadline.html"> Deadline </a></li>
							<li><a href="organizers.html"> Organizers </a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Featured Post -->
							<article class="post featured">

							<h3> Updates and News </h3>
                            <p style="text-align:justify; text-justify:inter-ideograph;">
							<b>Big News !!! </b> The top 3 teams in each track will receive cash rewards. 
							The details of the rewards are as follows:
							<b>Top-1: 3000$,
							Top-2: 1500$,
							Top-3: 800$.</b>
							See more details of ranking rules in <a href="elements.html"> Evaluation </a>.
							</p>
							<br>
                            
							<h3> Issues and Questions </h3>

							<p>
							For any questions, please send an email to lattehuang@tencent.com.
							You can also join our WeChat group by scanning the code below:
							<br>
							</p>
							<img src="images/pic4.png" width="20%" align="center" />
							<br>

                            



							<h3>
								Registration
							</h3>
							<p style="text-align:justify; text-justify:inter-ideograph;">
								
								We use <a href="https://codalab.lisn.upsaclay.fr/">CodaLab</a> for the registration.
								The sites for registration are <a href = "https://codalab.lisn.upsaclay.fr/competitions/2415#participate">subtaks1</a>, <a href = "https://codalab.lisn.upsaclay.fr/competitions/2417#participate">subtaks2</a>,
								<a href = "https://codalab.lisn.upsaclay.fr/competitions/2418#participate">subtaks3</a>.
								If you do not have the account of CodaLab, please create an account.
                                Here, we provide an example to help participants to complete the registration of the subtask1.
								<br> 
								<b>Step 1:</b> Click the <a href = "https://codalab.lisn.upsaclay.fr/competitions/2415#participate">subtask1</a>. 
								<br>
								<img src="images/subtask1.png" width="50%" align="center" />
                                <br>
								<b>Step 2:</b> Click the button <b>Participate</b>.
								<br>
								<img src="images/partipate.png" width="50%" align="center" />
								<br>
                                <b>Step 3:</b> Accept the terms and conditions.
								<br>
								<img src="images/accept.png" width="50%" align="center" />
								<br>
								<b>Step 4:</b> Click the button <b>Register</b>.
								<br>
								<img src="images/res.png" width="50%" align="center" />
								<br>
								Then, the website turns to:
								<br>
								<img src="images/final.png" width="50%" align="center" />
								<br>
								It means that you are successfully completing the registration.
							    Subtask2 and Subtask3 are the same as Subtask1.
							    <br>
								And we will process your registration as soon as possible. 
								Upon we accept your registration, you will see the link for the training data.
								<br>
								<img src="images/data.png" width="50%" align="center" />
								<!--2022-03-04: Training and validation data has been released <a href="https://drive.google.com/drive/folders/1d1n-A1_dw2cfntt576wyJLG8GMp2A_B0?usp=sharing"> Google Drive</a> <a href="https://pan.baidu.com/s/12GtbAgjwd_Bpp3DxU94QFQ">Baidu Drive</a> (Extraction code: 0aka). 
								Paticipants can use the released data to develop their algorithms.-->
								<br>
							</p>

							<h3>
								Submission
							</h3>

							<p>
							We use <a href="https://codalab.lisn.upsaclay.fr/">CodaLab</a> for online submission in the development phase. 
							Here, we provide an example to help participants to format their submissions. 
							The sites are for the <a href="https://codalab.lisn.upsaclay.fr/competitions/2415">subtask1 </a>, <a href="https://codalab.lisn.upsaclay.fr/competitions/2417"> subtask2</a> and <a href="https://codalab.lisn.upsaclay.fr/competitions/2418"> subtask3</a>, respectively.
							</p>
	
						    <h3> Introduction </h3>
							<p style="text-align:justify; text-justify:inter-ideograph;"> 
									In recent years, with the rapid rise of short videos, live broadcastsand other media-based applications, the widespread transmission of video data has led to a significant
									increase in user-generated content. 
									A wide variety of creative platforms and pattern have emerged, and media publication criteria is becoming more and more civilian,
									leading to more complex and dynamic acoustic scenes in various long or short video and live streaming. 
									The problems of video subtitle recognition and speech recognition under various scenarios have been of considerable concern by researchers. 
									The development of methods accurately recognize and understand various types of video content has become an indispensable tool in downstream applications such as subtitle creation, content recommendation, digital media
									archival, and so on. 
									In this challenge, we focus on extracting subtitles from videos using both visual and audio modalities. Most previous works exploit it with a single modality , and each modality has its own
									advantages and annotation difficulties on various types of training data. 
									To this end, we present tasks that explore the integration of the advantages of both video and audio modalitioes. 
									We expect to obtain high precision subtitles with lower annotation costs, thus expanding the scope of practical applications.
							</p>
								
					
						<h3> Description of the Challenge </h3>
					    <p style="text-align:justify; text-justify:inter-ideograph;">
						<b> Subtask 1: Extracting subtitles in visual modality with audio annotations </b>
						<br>
						To extract subtitles from video frames, a large number of keyframes should be
						annotated with bounding boxes and contents, which is extremely costive. However,
						speech transcripts are much easier to obtain, and they contain almost all the content of
						the subtitles. In this subtask, we present a challenge that explores learning visual
						subtitles with the supervision of speech scripts. We expect that the annotation from
						the audio modality can improve the subtitle extraction from the visual modality.
						In this subtask, we will present 75h of video content, divided into set of 50, 5, 20 as
						training, validation, and testing sets, respectively. For the training set, only audio
						annotations will be provided. The participants are required to design subtitles OCR
						system with these annotations. To pretrain an OCR system, participants can also use a
						limited number of open datasets, and fine-tune their model with audio supervision.
						Under these conditions, will be asked to produce subtitle text for each video in our
						testing set, and the submitted results will be ranked using the CER metric.
						The site for the submission is <a href="https://codalab.lisn.upsaclay.fr/competitions/2415">subtask1</a>.
						</p>
						
						<p style="text-align:justify; text-justify:inter-ideograph;">
						<b> Subtask 2: Extracting subtitles in audio modal with visual annotations </b>
						<br>
						In speech recognition tasks, especially in video speech recognition tasks, audio data
						are difficult to label owing to background music, sound effects, or noise. However, all
						texts information including subtitles in the video can supply weakly labeled
						information.
						There are lots of videos with subtitles. Some texts information including
						subtitles are manually generated, whereas some are automaticly generated.
						Although the quality of automatic subtitles may be worse than that of manual subtitles, they are
						often available in much greater quantity.
 						Therefore, this task considers the use of
 						visual annotations in videos, especially automatic annotations to assist in building an
						ASR system.
						In this subtask, the participants will be required to use only visual annotations to
						build an ASR system for the corresponding videos. To improve the robustness, some
						public ASR data in the following tables may be used as well. We will also provide a
						baseline model. The submitted results will be ranked with the CER metric on our
						testing set.
						The site for the submission is <a href="https://codalab.lisn.upsaclay.fr/competitions/2417">subtask2</a>.
						</p>
                        
						<p style="text-align:justify; text-justify:inter-ideograph;">
							<b>Subtask 3: Extracting subtitles with both visual and audio annotations</b>
							<br>
							In this subtask, for the training set, we present 50 hours of video content with both the
							visual and audio supervisions and 200-hour video content with no annotation.
							Another 20 and 5 h of videos will be provided to serve as validation and testing sets, respectively. 
							For the visual annotation, we will provide characters of all text in key frames, we will present speech transcripts of
							each VAD segment or the audio modal. With these data, participants will be required to produce
							 subtitle for each video in our testing set, and the submitted results will be ranked with
							the CER metric.
							The site for the submission is <a href="https://codalab.lisn.upsaclay.fr/competitions/2418">subtask3</a>.
						</p>

						<h3> Datasets </h3>
						<p style="text-align:justify; text-justify:inter-ideograph;"> 
							We present a large-scale video dataset with 75 hours of video content, among which
							50/5/20 hours are used for training, validation, and testing, respectively. Both visual
							(weak) and audio annotations are provided. Moreover, additional 200-hour unlabeled
							video content is provided as an unsupervised training resources.
						</p>
						
                            <p style="text-align:justify; text-justify:inter-ideograph;"> 
								<b> Visual annotation: </b>
                                <br>
								For each video, we will provide pseudo-subtitles along with their locations and
								timestamps. In the creation stage, our video OCR system results are generated and
								corrected in combined with ASR ground truth labels as follows:
								<br/>
								<b>Step 1:</b> We extract five frames per second from videos, and then detect and recognize
							   the text in the frames with the high-precision TencentOCR systems. We save all the
							   text lines as the visual annotations, and they are used for the subtask 2.
							   <br/>
							   <b>Step 2:</b> To identify the subtitle text, we compare the OCR results with the ASR
							   ground truth to determine which text lines belonge to the subtitles, and take the
							   corresponding bounding boxes, and recognized text as the subtitle pseudo-
							   annotations.
							   <br/>
							   The location is presented as a bounding box with the coordinates of the four
							   corners. The timestamp can be obtained with the frames per second (FPS) of the video
							   and the index of each frame.
							   <br/>
							   The annotation has the following format for a subtitle:
							   <br/>
							   {
								<br>
								&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "video_name": video_name, 
								<br>
								&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "frame_id": frame_id,
								<br>
								&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "bounding_box": bounding_box, 
								 <br>
								&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "text": text,
								<br>
								}.
								<br/>
								For example, for a video named "TV_00000001", all of the texts in a frame, including one subtitle in the red box, has annotations shown belows :
								<br/>
							</p>
							<p style="text-align:center;">
								<img src="images/pic1.png" width="50%" align="center" />
							</p>
							<p style="text-align:justify; text-justify:inter-ideograph;">
								{
									<br>
									&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "video_name": "TV_00000001",
                                    <br>
									&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "frame_id": 100,
									<br>
									&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "content":  {
									<br>
									&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "text":"BTV", 
									<br>
									&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "text":"北京同仁堂", 
									<br>
                                    &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "text":“冠名播出”,
									<br>
									&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "text": "都放在你这个手指的动作上面", },
									<br>
								}.

							</p>
							
							<p style="text-align:justify; text-justify:inter-ideograph;">
								<b> Audio annotation: </b>
                                <br>
								For each audio clip, we will provide their text and segment file in terms of the KALDI
								format (https://kaldi-asr.org/doc/data_prep.html). The segment file is the start and end
								time of VAD segments for each audio clip.
								<br/>
								<b>text:</b> TV_00000001 都 放在 你 这个 手指 的 动作 上面 
								<br/>
								<b>segments:</b> TV_00000001 TV 0 3.259
								<br/>
								<b>wav.scp:</b> TV_00000001 TV.wav							
							</p>

							<h3> Evaluation </h3>
							<p style="text-align:justify; text-justify:inter-ideograph;">
							The participants are required to output non-repeating subtitles and concatenate them
							for each video. For a given video, subtitles of adjacent frames may be the same, and
							the participants are required to deduplicate them and output subtitles. Normalized
							metric in terms of Normalized Edit Distance (1-N.E.D. specially) will be treated as
							the official ranking metric.
							We will evaluate the predicted transcription with
							the Normalized Edit Distance (N.E.D), which is formulated as:
							<br/>
							<img src="images/pic2.png" width="30%" align="center" />
							<br/>
							where D(:) stands for the Levenshtein Distance, and     and   denote the predicted
								subtitles in string and the corresponding ground truths for each video. N is the
								number of the test videoes.
							<br/>
                            <br>
							On March 12, organizers provide the training set with annotations. Each task has 50 hours of video data. 
							Participants are required to develop corresponding models according to the requirements of each track.
                            <br/>
                            <br>
							On April 22, the organizers will provide the validation set without annotations. And each track contains 20 hours of video data. 
							Participants predict subtitles of each video, and submit the prediction results to the CodaLab website. 
							The organizers will give the ranking on the validation set according to the prediction result.
                            <br/>
                            <br>
							On May 7th, the organizers will provide the test set (including 5 hours of video data) without annotations. 
							It is required that the top ten participants of each track in the validation set be within two days (that is, before May 9th). 
							According to the video data in the test set, <b> top ten participants on the validation set </b> are required to predict the subtitles in the video, 
							and submit the prediction results to the CodaLab website.
							Finally, <b> the final rank of each track = 50% * rank on the validation set + 50% * rank on the test set </b>. 
							The test set will be announced on the official website (icprmsr.github.io) after May 9, 
							and the final ranking information of the participants will also be announced on the competition website, 
							and the participants will be notified by email.

							
							<h3> Deadlines </h3>
                            <table >
								<tr>
									<td> <b> Milestone </b> </td>
									<td> <b> Date </b> </td>
								</tr>
								<tr>
									<td> Registration period </td>
									<td> Mar 7, 2022 -  </td>
								</tr>
								<tr>
									<td> Data release  </td>
									<td>  Mar 12, 2022 </td>
								</tr>
								<tr>
									<td> Development period </td>
									<td> Mar 12, 2022 - Apr 22, 2022 </td>
								</tr>
								<tr>
									<td> Evaluation period </td>
									<td> Apr 22, 2022 - May 7, 2022 </td>
								</tr>
								<tr>
									<td> Release of evaluation dataset </td>
									<td>  Apr 22, 2022 </td>
								</tr>
								<tr>
									<td> Deadline for submission of results </td>
									<td> May 7, 2022 </td>
								</tr>
								<tr>
									<td> Deadline for submission of method descriptions </td>
									<td> May 12, 2022 </td>
								</tr>
								<tr>
									<td> Announcement of evaluation results </td>
									<td> May 13, 2022 </td>
								</tr>
								<tr>
									<td> Competition paper submission </td>
									<td> May 20, 2022 </td>
								</tr>
							</table>

                            

							<br>
							<h3>
								Contact information for the organizers
							</h3>
							<p style="text-align:justify; text-justify:inter-ideograph;">
								Shan Huang, Tencent, lattehuang@tencent.com <br>
								Shen Huang, Tencent, springhuang@tencent.com <br>
								Li Lu, Tencent, adolphlu@tencent.com <br> 
								Pengfei Hu, Tencent, alanpfhu@tencent.com <br>
								Xiang Wang, Tencent, andyxwang@tencent.com <br>
								Jian kang, Tencent, jiankang@tencent.com <br>
								Lianwen Jin, South China University of Technology, eelwjin@scut.edu.cn <br>
								YuLiang Liu, Huazhong University of Science and Technology, ylliu@hust.edu.cn <br>
								Yaqiang Wu, Lenovo, wuyqe@lenovo.com <br>
							</p>
							
                            <img src="images/org_logo.png" width="95%" align="center" />
							<br>
							<br>
							<a href="https://www.36dianping.com/ " class="img-box">
								<img src="images/36kr_banner.png" />
							</a>
					
						</article>
					</div>
			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>